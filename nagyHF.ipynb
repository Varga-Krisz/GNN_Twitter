{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "TODO: Leírást gyártani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -q\n",
    "\n",
    "# pip didn't find pyg-lib for some reason\n",
    "#!pip install pyg-lib>=0.4 torch-scatter>=2.1 torch-sparse>=0.6 torch-cluster>=1.6 torch-spline_conv>=1.2 -f https://data.pyg.org/whl/torch-2.5.0+cu121.html --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ego networks\n",
    "!wget -P /Data https://snap.stanford.edu/data/twitter.tar.gz\n",
    "!unzip -qq /Data/twitter.tar.gz\n",
    "!rm twitter.tar.gz\n",
    "\n",
    "# Edge list\n",
    "!wget -P /Data https://snap.stanford.edu/data/twitter_combined.txt.gz\n",
    "!unzip /Data/twitter_combined.txt.gz\n",
    "!rm twitter_combined.txt.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0 in PyTorch version 2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Graph\n",
    "import networkx as nx\n",
    "import torch_geometric as pyg\t# This import doesn't work\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Default device breaks node and edge sampling\n",
    "#torch.set_default_device(DEVICE)\n",
    "\n",
    "print(f\"Training on {DEVICE} in PyTorch version {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100318079', '10146102', '101859065', '101903164', '102765423']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get nodeID's with associated files (ego nodes)\n",
    "nodes = []\n",
    "for file in os.listdir(\"./Data/twitter\"):\n",
    "\tif file.split(\".\")[0] not in nodes:\n",
    "\t\tnodes.append(file.split(\".\")[0])\n",
    "nodes[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 10 out of 973\n",
      "File 20 out of 973\n",
      "File 30 out of 973\n",
      "File 40 out of 973\n",
      "File 50 out of 973\n",
      "File 60 out of 973\n",
      "File 70 out of 973\n",
      "File 80 out of 973\n",
      "File 90 out of 973\n",
      "File 100 out of 973\n",
      "File 110 out of 973\n",
      "File 120 out of 973\n",
      "File 130 out of 973\n",
      "File 140 out of 973\n",
      "File 150 out of 973\n",
      "File 160 out of 973\n",
      "File 170 out of 973\n",
      "File 180 out of 973\n",
      "File 190 out of 973\n",
      "File 200 out of 973\n",
      "File 210 out of 973\n",
      "File 220 out of 973\n",
      "File 230 out of 973\n",
      "File 240 out of 973\n",
      "File 250 out of 973\n",
      "File 260 out of 973\n",
      "File 270 out of 973\n",
      "File 280 out of 973\n",
      "File 290 out of 973\n",
      "File 300 out of 973\n",
      "File 310 out of 973\n",
      "File 320 out of 973\n",
      "File 330 out of 973\n",
      "File 340 out of 973\n",
      "File 350 out of 973\n",
      "File 360 out of 973\n",
      "File 370 out of 973\n",
      "File 380 out of 973\n",
      "File 390 out of 973\n",
      "File 400 out of 973\n",
      "File 410 out of 973\n",
      "File 420 out of 973\n",
      "File 430 out of 973\n",
      "File 440 out of 973\n",
      "File 450 out of 973\n",
      "File 460 out of 973\n",
      "File 470 out of 973\n",
      "File 480 out of 973\n",
      "File 490 out of 973\n",
      "File 500 out of 973\n",
      "File 510 out of 973\n",
      "File 520 out of 973\n",
      "File 530 out of 973\n",
      "File 540 out of 973\n",
      "File 550 out of 973\n",
      "File 560 out of 973\n",
      "File 570 out of 973\n",
      "File 580 out of 973\n",
      "File 590 out of 973\n",
      "File 600 out of 973\n",
      "File 610 out of 973\n",
      "File 620 out of 973\n",
      "File 630 out of 973\n",
      "File 640 out of 973\n",
      "File 650 out of 973\n",
      "File 660 out of 973\n",
      "File 670 out of 973\n",
      "File 680 out of 973\n",
      "File 690 out of 973\n",
      "File 700 out of 973\n",
      "File 710 out of 973\n",
      "File 720 out of 973\n",
      "File 730 out of 973\n",
      "File 740 out of 973\n",
      "File 750 out of 973\n",
      "File 760 out of 973\n",
      "File 770 out of 973\n",
      "File 780 out of 973\n",
      "File 790 out of 973\n",
      "File 800 out of 973\n",
      "File 810 out of 973\n",
      "File 820 out of 973\n",
      "File 830 out of 973\n",
      "File 840 out of 973\n",
      "File 850 out of 973\n",
      "File 860 out of 973\n",
      "File 870 out of 973\n",
      "File 880 out of 973\n",
      "File 890 out of 973\n",
      "File 900 out of 973\n",
      "File 910 out of 973\n",
      "File 920 out of 973\n",
      "File 930 out of 973\n",
      "File 940 out of 973\n",
      "File 950 out of 973\n",
      "File 960 out of 973\n",
      "File 970 out of 973\n",
      "0                      \n",
      "1                     1\n",
      "2          10bandsilove\n",
      "3                     2\n",
      "4                    20\n",
      "              ...      \n",
      "155517    puraseduction\n",
      "155518           rrrafo\n",
      "155519       saumenscch\n",
      "155520      sheisunreal\n",
      "155521    twstdsymphony\n",
      "Length: 155521, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'coachella': 1, 'gabrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'charlieroseshow': 1, 'claychristensen': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'jack': 1, 'makeitwork'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'jack': 1, 'jbrewer': 1, 'twitter': 1, 'twitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'foodtruck': 1, 'gzahnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568552194</th>\n",
       "      <td>{'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627575</th>\n",
       "      <td>{'lucasgomezbr': 1, 'natystadler': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568655523</th>\n",
       "      <td>{'dailyprofet': 1, 'paixaojovem': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568699879</th>\n",
       "      <td>{'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568770231</th>\n",
       "      <td>{'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120707 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Features\n",
       "12         {'claychristensen': 1, 'coachella': 1, 'gabrie...\n",
       "12         {'charlieroseshow': 1, 'claychristensen': 1, '...\n",
       "12         {'claychristensen': 1, 'jack': 1, 'makeitwork'...\n",
       "12         {'jack': 1, 'jbrewer': 1, 'twitter': 1, 'twitt...\n",
       "12         {'claychristensen': 1, 'foodtruck': 1, 'gzahnd...\n",
       "...                                                      ...\n",
       "568552194  {'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...\n",
       "568627575              {'lucasgomezbr': 1, 'natystadler': 1}\n",
       "568655523               {'dailyprofet': 1, 'paixaojovem': 1}\n",
       "568699879  {'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...\n",
       "568770231  {'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...\n",
       "\n",
       "[120707 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All feat names in order of discovery\n",
    "allfeatnames = pd.Series(dtype=str)\n",
    "# Node features as dictionaries of followed #-s and @-s (with multiplicities)\n",
    "allfeats = pd.DataFrame(columns=[\"Features\"])\n",
    "c = 0\n",
    "\n",
    "for node in nodes:\n",
    "\tc+=1\n",
    "\tif not c%10:\n",
    "\t\tprint(f\"File {c} out of {len(nodes)}\")\n",
    "\n",
    "\t# Remove duplicates with special characters and lowercase-uppercase shenanigans\n",
    "\tfeatnames = pd.read_csv(f\"./Data/twitter/{node}.featnames\", sep=\" \", header=None)[1].str.rstrip(\".,;:'+-!?)(}{][\").str.lstrip(\"#@\").str.lower()\n",
    "\tallfeatnames = pd.Series(pd.concat([allfeatnames, featnames]).unique())\n",
    "\n",
    "\t# Feature vectors except ego node\n",
    "\tfeatdf = pd.read_csv(f\"./Data/twitter/{node}.feat\", sep=\" \", header=None).set_index(0).set_axis(featnames, axis=1)\n",
    "\n",
    "\t# Feature vector of ego node\n",
    "\tegofeat = pd.read_csv(f\"./Data/twitter/{node}.egofeat\", sep=\" \", header=None).set_axis(featnames, axis=1)\n",
    "\tegofeat = pd.concat([pd.DataFrame([int(node)], columns=[0]), egofeat], axis=1).set_index(0)\n",
    "\n",
    "\t# All feature vectors in a DataFrame\n",
    "\tfeatdf = pd.concat([egofeat, featdf])\n",
    "\tfeatdf = featdf.T.groupby(level=0).sum().T\n",
    "\n",
    "\t# Create bag-of-words style dictionaries to later turn into tensors\n",
    "\tfeatdf[\"Features\"] = [{col: featdf.at[idx, col] for col in featdf.columns if featdf.at[idx, col]} for idx in featdf.index]\n",
    "\t\n",
    "\t# Concatenate\n",
    "\tallfeats = pd.concat([allfeats, featdf.Features])\n",
    "\n",
    "\t# Edges - use twitter_combined instead\n",
    "\t# edgedf = pd.read_csv(f\"./Data/twitter/{node}.edges\", sep=\" \", header=None).T\n",
    "\t\n",
    "# Drop the (one) empty string and save\n",
    "allfeatnames = allfeatnames.dropna()\n",
    "allfeatnames.to_json(\"feature_names.json\")\n",
    "print(allfeatnames)\n",
    "\n",
    "# Drop the empty dicts (there's over 10.000 unique nodes following nothing)\n",
    "allfeats = allfeats.loc[allfeats[\"Features\"] != {}].sort_index()\n",
    "allfeats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Bag_of_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'coachella': 1, 'gabrie...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'amyquispe': 1, 'baratunde': 1, 'busterbenson...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568552194</th>\n",
       "      <td>{'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...</td>\n",
       "      <td>568552194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627575</th>\n",
       "      <td>{'lucasgomezbr': 1, 'natystadler': 1}</td>\n",
       "      <td>568627575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568655523</th>\n",
       "      <td>{'dailyprofet': 1, 'paixaojovem': 1}</td>\n",
       "      <td>568655523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568699879</th>\n",
       "      <td>{'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...</td>\n",
       "      <td>568699879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568770231</th>\n",
       "      <td>{'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...</td>\n",
       "      <td>568770231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70923 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Features  Bag_of_Words\n",
       "12         {'claychristensen': 1, 'coachella': 1, 'gabrie...            12\n",
       "13         {'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...            13\n",
       "17         {'amyquispe': 1, 'baratunde': 1, 'busterbenson...            17\n",
       "20         {'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...            20\n",
       "47         {'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...            47\n",
       "...                                                      ...           ...\n",
       "568552194  {'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...     568552194\n",
       "568627575              {'lucasgomezbr': 1, 'natystadler': 1}     568627575\n",
       "568655523               {'dailyprofet': 1, 'paixaojovem': 1}     568655523\n",
       "568699879  {'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...     568699879\n",
       "568770231  {'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...     568770231\n",
       "\n",
       "[70923 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# allfeats contains many duplicate indices which we need to deal with\n",
    "\n",
    "# Function to aggregate the dicts of duplicate indices, taking the maximum of all multiplicities\n",
    "# (The feature vectors are different across files)\n",
    "def dict_aggregation(dicts):\n",
    "\tout = {}\n",
    "\tfor d in dicts:\n",
    "\t\tfor k,v in d.items():\n",
    "\t\t\tif k in out.keys() and out[k] >= v:\n",
    "\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\tout[k] = v\n",
    "\treturn out\n",
    "\n",
    "# Update node dicts with duplicate indices, then remove unnecessary duplicates\n",
    "for idx in allfeats[allfeats.index.duplicated()].index.unique():\n",
    "\tagg = dict_aggregation(allfeats.loc[idx].Features)\n",
    "\tallfeats.loc[idx, \"Features\"] = [agg] * len(allfeats.loc[idx])\n",
    "\n",
    "allfeats = allfeats[~allfeats.index.duplicated()]\n",
    "allfeats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_size:  194\n",
      "About 2.48% done\n",
      "About 2.48% done\n",
      "About 2.49% done\n",
      "About 2.51% done\n",
      "About 2.59% done\n",
      "About 2.66% done\n",
      "About 2.70% done\n",
      "About 2.75% done\n",
      "About 2.78% done\n",
      "About 2.80% done\n",
      "About 3.01% done\n",
      "About 3.02% done\n",
      "About 3.03% done\n",
      "About 3.28% done\n",
      "About 3.30% done\n",
      "About 3.37% done\n",
      "About 3.53% done\n",
      "About 3.66% done\n",
      "About 3.77% done\n",
      "About 3.91% done\n",
      "About 4.31% done\n",
      "About 4.32% done\n",
      "About 4.34% done\n",
      "About 4.58% done\n",
      "About 4.65% done\n",
      "About 4.80% done\n",
      "About 4.80% done\n",
      "About 4.80% done\n",
      "About 5.63% done\n",
      "About 6.02% done\n",
      "About 6.95% done\n",
      "About 6.96% done\n",
      "About 7.76% done\n",
      "About 8.04% done\n",
      "About 8.38% done\n",
      "About 9.75% done\n",
      "About 10.26% done\n",
      "About 10.67% done\n",
      "About 12.20% done\n",
      "About 12.97% done\n",
      "About 13.15% done\n",
      "About 13.39% done\n",
      "About 14.10% done\n",
      "About 16.38% done\n",
      "About 17.40% done\n",
      "About 18.14% done\n",
      "About 22.18% done\n",
      "About 22.23% done\n",
      "About 22.46% done\n",
      "About 23.08% done\n",
      "About 23.96% done\n",
      "About 29.03% done\n",
      "About 34.05% done\n",
      "About 34.59% done\n",
      "About 34.84% done\n",
      "About 35.67% done\n",
      "About 37.26% done\n",
      "About 38.39% done\n",
      "About 42.28% done\n",
      "About 42.56% done\n",
      "About 49.01% done\n",
      "About 49.18% done\n",
      "About 53.97% done\n",
      "About 60.83% done\n",
      "About 63.43% done\n",
      "About 67.55% done\n",
      "About 87.73% done\n",
      "About 96.02% done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krisztián\\AppData\\Local\\Temp\\ipykernel_14860\\769440309.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[tensor([ 31517,     86, 115673,   6741,   6741,  25306,   7622,  25768,  13811,\n",
      "         115629,   7120,   1681,  45262,  24086,   8859,   7310,  45288,  45228,\n",
      "          25336,  25336,  31027,   2976,   2976,  19157,  88730,  24332,  38647,\n",
      "          25698,  51775,  59974,  60065,  60010,  60015,  39992,   3173,   3122,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0])\n",
      " tensor([ 3462, 11073, 24296, 24326, 24371,  3690, 27037,  7678,  2824, 46801,\n",
      "          4024, 24267, 60107, 51870, 99549, 99549, 32071,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0])\n",
      " tensor([70048,  6685, 24196, 70065, 25493, 17615, 11073, 25597, 51914, 24462,\n",
      "         70138, 24538,  6803,  6803, 51981,  6374,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0])\n",
      " ...\n",
      " tensor([14243, 56482,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0])\n",
      " tensor([   676,    306,    318,    318, 135889,    567,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0])\n",
      " tensor([14028, 14038, 47269, 47269,  5516,  3697, 14082,  2109,  2109,  9728,\n",
      "          9728, 19705,  5322, 29928, 12183,  2272, 14113,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0])                                  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  allfeats.loc[:, 'Bag_of_Words'] = allfeats.Bag_of_Words.apply(id2bow)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Bag_of_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'coachella': 1, 'gabrie...</td>\n",
       "      <td>[tensor(31517), tensor(86), tensor(115673), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...</td>\n",
       "      <td>[tensor(3462), tensor(11073), tensor(24296), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'amyquispe': 1, 'baratunde': 1, 'busterbenson...</td>\n",
       "      <td>[tensor(70048), tensor(6685), tensor(24196), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...</td>\n",
       "      <td>[tensor(24104), tensor(51836), tensor(25369), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...</td>\n",
       "      <td>[tensor(25362), tensor(25367), tensor(25368), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568552194</th>\n",
       "      <td>{'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...</td>\n",
       "      <td>[tensor(148053), tensor(130385), tensor(148073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627575</th>\n",
       "      <td>{'lucasgomezbr': 1, 'natystadler': 1}</td>\n",
       "      <td>[tensor(67889), tensor(67751), tensor(0), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568655523</th>\n",
       "      <td>{'dailyprofet': 1, 'paixaojovem': 1}</td>\n",
       "      <td>[tensor(14243), tensor(56482), tensor(0), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568699879</th>\n",
       "      <td>{'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...</td>\n",
       "      <td>[tensor(676), tensor(306), tensor(318), tensor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568770231</th>\n",
       "      <td>{'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...</td>\n",
       "      <td>[tensor(14028), tensor(14038), tensor(47269), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70923 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Features  \\\n",
       "12         {'claychristensen': 1, 'coachella': 1, 'gabrie...   \n",
       "13         {'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...   \n",
       "17         {'amyquispe': 1, 'baratunde': 1, 'busterbenson...   \n",
       "20         {'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...   \n",
       "47         {'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...   \n",
       "...                                                      ...   \n",
       "568552194  {'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...   \n",
       "568627575              {'lucasgomezbr': 1, 'natystadler': 1}   \n",
       "568655523               {'dailyprofet': 1, 'paixaojovem': 1}   \n",
       "568699879  {'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...   \n",
       "568770231  {'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...   \n",
       "\n",
       "                                                Bag_of_Words  \n",
       "12         [tensor(31517), tensor(86), tensor(115673), te...  \n",
       "13         [tensor(3462), tensor(11073), tensor(24296), t...  \n",
       "17         [tensor(70048), tensor(6685), tensor(24196), t...  \n",
       "20         [tensor(24104), tensor(51836), tensor(25369), ...  \n",
       "47         [tensor(25362), tensor(25367), tensor(25368), ...  \n",
       "...                                                      ...  \n",
       "568552194  [tensor(148053), tensor(130385), tensor(148073...  \n",
       "568627575  [tensor(67889), tensor(67751), tensor(0), tens...  \n",
       "568655523  [tensor(14243), tensor(56482), tensor(0), tens...  \n",
       "568699879  [tensor(676), tensor(306), tensor(318), tensor...  \n",
       "568770231  [tensor(14028), tensor(14038), tensor(47269), ...  \n",
       "\n",
       "[70923 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to create a Bag of Words tensor given an index\n",
    "def id2bow(index, bag_size = 195):\n",
    "\tbow = torch.zeros(bag_size, dtype=torch.int64)\n",
    "\tdictionary = allfeats.loc[index, \"Features\"]\n",
    "\tj = 0\n",
    "\tfor key,value in dictionary.items():\n",
    "\t\t# Accounts for multiplicities\n",
    "\t\tbow[j:j+value] = allfeatnames[allfeatnames == key].index.item()\n",
    "\t\tj += value\n",
    "\t# Occasional progress check\n",
    "\t# Note: Remaining index values are not uniformly distributed\n",
    "\tif not index%1000:\n",
    "\t\tprint(f\"About {index/5687702:.2f}% done\")\n",
    "\treturn bow\n",
    "\n",
    "# bag_size should be at least this (set one higher just to be sure)\n",
    "print(\"bag_size: \", allfeats.Features.apply(lambda x: sum(x.values())).max())\n",
    "\n",
    "# Set column same as index and apply id2bow\n",
    "allfeats[\"Bag_of_Words\"] = allfeats.index\n",
    "allfeats.loc[:, 'Bag_of_Words'] = allfeats.Bag_of_Words.apply(id2bow)\n",
    "allfeats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Bag_of_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'coachella': 1, 'gabrie...</td>\n",
       "      <td>[tensor(31517), tensor(86), tensor(115673), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...</td>\n",
       "      <td>[tensor(3462), tensor(11073), tensor(24296), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'amyquispe': 1, 'baratunde': 1, 'busterbenson...</td>\n",
       "      <td>[tensor(70048), tensor(6685), tensor(24196), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...</td>\n",
       "      <td>[tensor(24104), tensor(51836), tensor(25369), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...</td>\n",
       "      <td>[tensor(25362), tensor(25367), tensor(25368), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568552194</th>\n",
       "      <td>{'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...</td>\n",
       "      <td>[tensor(148053), tensor(130385), tensor(148073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627575</th>\n",
       "      <td>{'lucasgomezbr': 1, 'natystadler': 1}</td>\n",
       "      <td>[tensor(67889), tensor(67751), tensor(0), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568655523</th>\n",
       "      <td>{'dailyprofet': 1, 'paixaojovem': 1}</td>\n",
       "      <td>[tensor(14243), tensor(56482), tensor(0), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568699879</th>\n",
       "      <td>{'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...</td>\n",
       "      <td>[tensor(676), tensor(306), tensor(318), tensor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568770231</th>\n",
       "      <td>{'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...</td>\n",
       "      <td>[tensor(14028), tensor(14038), tensor(47269), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70923 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Features  \\\n",
       "12         {'claychristensen': 1, 'coachella': 1, 'gabrie...   \n",
       "13         {'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...   \n",
       "17         {'amyquispe': 1, 'baratunde': 1, 'busterbenson...   \n",
       "20         {'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...   \n",
       "47         {'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...   \n",
       "...                                                      ...   \n",
       "568552194  {'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...   \n",
       "568627575              {'lucasgomezbr': 1, 'natystadler': 1}   \n",
       "568655523               {'dailyprofet': 1, 'paixaojovem': 1}   \n",
       "568699879  {'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...   \n",
       "568770231  {'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...   \n",
       "\n",
       "                                                Bag_of_Words  \n",
       "12         [tensor(31517), tensor(86), tensor(115673), te...  \n",
       "13         [tensor(3462), tensor(11073), tensor(24296), t...  \n",
       "17         [tensor(70048), tensor(6685), tensor(24196), t...  \n",
       "20         [tensor(24104), tensor(51836), tensor(25369), ...  \n",
       "47         [tensor(25362), tensor(25367), tensor(25368), ...  \n",
       "...                                                      ...  \n",
       "568552194  [tensor(148053), tensor(130385), tensor(148073...  \n",
       "568627575  [tensor(67889), tensor(67751), tensor(0), tens...  \n",
       "568655523  [tensor(14243), tensor(56482), tensor(0), tens...  \n",
       "568699879  [tensor(676), tensor(306), tensor(318), tensor...  \n",
       "568770231  [tensor(14028), tensor(14038), tensor(47269), ...  \n",
       "\n",
       "[70923 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save\n",
    "allfeats.to_pickle(\"feature_bow.pkl\")\n",
    "allfeats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, creating and splitting the data into dataloaders, defining train and test functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Bag_of_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'coachella': 1, 'gabrie...</td>\n",
       "      <td>[tensor(31517), tensor(86), tensor(115673), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...</td>\n",
       "      <td>[tensor(3462), tensor(11073), tensor(24296), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'amyquispe': 1, 'baratunde': 1, 'busterbenson...</td>\n",
       "      <td>[tensor(70048), tensor(6685), tensor(24196), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...</td>\n",
       "      <td>[tensor(24104), tensor(51836), tensor(25369), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...</td>\n",
       "      <td>[tensor(25362), tensor(25367), tensor(25368), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Features  \\\n",
       "12  {'claychristensen': 1, 'coachella': 1, 'gabrie...   \n",
       "13  {'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...   \n",
       "17  {'amyquispe': 1, 'baratunde': 1, 'busterbenson...   \n",
       "20  {'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...   \n",
       "47  {'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...   \n",
       "\n",
       "                                         Bag_of_Words  \n",
       "12  [tensor(31517), tensor(86), tensor(115673), te...  \n",
       "13  [tensor(3462), tensor(11073), tensor(24296), t...  \n",
       "17  [tensor(70048), tensor(6685), tensor(24196), t...  \n",
       "20  [tensor(24104), tensor(51836), tensor(25369), ...  \n",
       "47  [tensor(25362), tensor(25367), tensor(25368), ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load files created previously\n",
    "\n",
    "#allfeatnames = pd.read_json(\"feature_names.json\", typ=\"series\", dtype=str)\n",
    "#print(allfeatnames.head())\n",
    "\n",
    "# Number of different feature names for the embedding size\n",
    "#emb_size = len(allfeatnames)+1\n",
    "emb_size = 155522\n",
    "\n",
    "allfeats = pd.read_pickle(\"feature_bow.pkl\")\n",
    "allfeats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge list:\n",
      "            0          1\n",
      "0  214328887   34428380\n",
      "1   17116707   28465635\n",
      "2  380580781   18996905\n",
      "3  221036078  153460275\n",
      "4  107830991   17868918\n",
      "Complete graph nodes and edges:  81306 1768149\n",
      "Restricted graph nodes and edges:  70923 1655299\n",
      "Features of node index 12:\n",
      " {'BoW': tensor([ 31517,     86, 115673,   6741,   6741,  25306,   7622,  25768,  13811,\n",
      "        115629,   7120,   1681,  45262,  24086,   8859,   7310,  45288,  45228,\n",
      "         25336,  25336,  31027,   2976,   2976,  19157,  88730,  24332,  38647,\n",
      "         25698,  51775,  59974,  60065,  60010,  60015,  39992,   3173,   3122,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0], device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 1655299], BoW=[70923, 195], num_nodes=70923)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create graph: from pandas to NetworkX to torch_geometric data object\n",
    "# Complete edge list\n",
    "edge_list = pd.read_csv(\"./Data/twitter_combined.txt\", sep=\" \", header=None)\n",
    "print(\"Edge list:\\n\", edge_list.head())\n",
    "\n",
    "# Complete graph\n",
    "G = nx.from_pandas_edgelist(edge_list, 0, 1, create_using=nx.DiGraph)\n",
    "print(\"Complete graph nodes and edges: \", G.number_of_nodes(), G.number_of_edges())\n",
    "\n",
    "# Restrict to nonzero feature nodes\n",
    "G = G.subgraph(allfeats.index)\n",
    "print(\"Restricted graph nodes and edges: \", G.number_of_nodes(), G.number_of_edges())\n",
    "\n",
    "# Give each node its it's bag of words as attribute\n",
    "nx.set_node_attributes(G, {idx: allfeats.Bag_of_Words[idx].to(DEVICE) for idx in allfeats.index}, \"BoW\")\n",
    "\n",
    "# Example\n",
    "print(\"Features of node index 12:\\n\", G.nodes[12])\n",
    "\n",
    "# Create the graph with a bag of words as the only node attribute\n",
    "graph = from_networkx(G)\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data: Apply train-val-test masks and create dataloaders\n",
    "\n",
    "transform = RandomLinkSplit(num_val=0.1, num_test=0.1)\n",
    "train_data, val_data, test_data = transform(graph)\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=[20, 20],\n",
    "    neg_sampling=NegativeSampling(\"binary\"),\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,\n",
    "    num_neighbors=[20, 20],\n",
    "    neg_sampling=NegativeSampling(\"binary\"),\n",
    "    batch_size=2048,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data,\n",
    "    num_neighbors=[20, 20],\n",
    "    neg_sampling=NegativeSampling(\"binary\"),\n",
    "    batch_size=2048,\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test functions\n",
    "\n",
    "# Display progress\n",
    "def progress(value, max=100):\n",
    "#\tif value==max:\n",
    "#\t\treturn \"\"\n",
    "\treturn HTML(f\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\")\n",
    "\n",
    "def train(epoch, model):\n",
    "\tmodel.train()\n",
    "\t\n",
    "\t# Reset progress\n",
    "\tbar.update(progress(0, len(train_loader)))\n",
    "\n",
    "\t\n",
    "\tepoch_loss = 0.0\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\n",
    "\tfor i, batch in enumerate(train_loader, 0):\n",
    "\t\t# Predict if user1 follows user2\n",
    "\t\t# .edge_label_index -> edges to predict\n",
    "\t\t# .edge_label -> 0-1 edge labels of edge_label_index\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = model(batch.to(DEVICE))\n",
    "\t\tloss = criterion(outputs, batch.edge_label.to(DEVICE))\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tcorrect += (outputs.round()==batch.edge_label).sum().item()\n",
    "\t\ttotal += len(outputs)\n",
    "\n",
    "\t\tepoch_loss += loss.item()\n",
    "\n",
    "\t\tbar.update(progress(i+1, len(train_loader)))\n",
    "\t\n",
    "\tepoch_loss /= len(train_loader)\n",
    "\tepoch_acc = correct/total\t\t\n",
    "\treturn epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def eval(epoch, model, dataloader=val_loader):\n",
    "\tmodel.eval()\n",
    "\n",
    "\t# Reset progress\n",
    "\tbar.update(progress(0, len(dataloader)))\n",
    "\n",
    "\tepoch_loss = 0.0\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\n",
    "\tfor i, batch in enumerate(dataloader, 0):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = model(batch.to(DEVICE))\n",
    "\t\t\tloss = criterion(outputs, batch.edge_label.to(DEVICE))\n",
    "\t\t\tcorrect += (outputs.round()==batch.edge_label).sum().item()\n",
    "\t\t\ttotal += len(outputs)\n",
    "\n",
    "\t\tepoch_loss += loss.item()\n",
    "\n",
    "\t\tbar.update(progress(i+1, len(dataloader)))\n",
    "\n",
    "\t\n",
    "\tepoch_loss /= len(dataloader)\n",
    "\tepoch_acc = correct/total\n",
    "\treturn epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a node2vec embedding of the graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run this block if we don't want to change the hyperparameters\n",
    "\n",
    "def n2v_train(numEpoch = 5,\n",
    "\t\t\t  batch_size = 128,\n",
    "\t\t\t  lr = 0.001,\n",
    "\t\t\t  **kwargs\n",
    "\t\t\t  ):\n",
    "\tn2v = gnn.Node2Vec(**kwargs).to(DEVICE)\n",
    "\tloader = n2v.loader(batch_size=batch_size, shuffle=True)\n",
    "\toptimizer = optim.Adam(n2v.parameters(), lr=lr)\n",
    "\n",
    "\tn2v.train()\n",
    "\tfor epoch in range(numEpoch):\n",
    "\t\tepoch_loss = 0\n",
    "\t\tfor pos_rw, neg_rw in loader:\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss = n2v.loss(pos_rw.to(DEVICE), neg_rw.to(DEVICE))\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tepoch_loss += loss.item()\n",
    "\t\tprint(f\"Epoch {epoch+1} loss: {epoch_loss/len(loader):.4f}\")\n",
    "\treturn n2v\n",
    "\n",
    "node2vec_trained = n2v_train(numEpoch=100,\n",
    "\t\t\t\t\t\t\t batch_size=512,\n",
    "\t\t\t\t\t\t\t edge_index = train_data.edge_index,\n",
    "\t\t\t\t\t\t\t embedding_dim = 20,\n",
    "\t\t\t\t\t\t\t walk_length = 10,\n",
    "\t\t\t\t\t\t\t context_size = 10,\n",
    "\t\t\t\t\t\t\t walks_per_node = 500,\n",
    "\t\t\t\t\t\t\t )\n",
    "\n",
    "torch.save(node2vec_trained.state_dict(), \"node2vec_trained.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krisztián\\AppData\\Local\\Temp\\ipykernel_18828\\1831015820.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(\"node2vec_trained.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load node2vec embedding\n",
    "state = torch.load(\"node2vec_trained.pt\")\n",
    "n2v = gnn.Node2Vec(train_data.edge_index, 20, 10, 10).to(DEVICE)\n",
    "n2v.load_state_dict(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models\n",
    "\n",
    "Not using the graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix factorization\n",
    "# Theory: People tend to follow people similar to them (i.e. form echo chambers)\n",
    "# Problem: Symmetric, which is not accurate for celebrities, for example (people with many followers)\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_features = emb_size, embedding_dim = 20):\n",
    "        super().__init__()\n",
    "        self.emb = nn.EmbeddingBag(num_features,\n",
    "                                   embedding_dim,\n",
    "                                   max_norm=1,\n",
    "                                   scale_grad_by_freq=True,\n",
    "                                   mode=\"sum\",\n",
    "                                   padding_idx=0,\n",
    "                                   )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        edge_index = data.edge_label_index\n",
    "        user1_feat, user2_feat = data.BoW[edge_index]\n",
    "\n",
    "        user1_vector = self.emb(user1_feat)\n",
    "        user2_vector = self.emb(user2_feat)\n",
    "        return (user1_vector * user2_vector).sum(1)+0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='783'\n",
       "            max='1294',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            783\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 1.1404, - accuracy: 0.1498\n",
      "Epoch 1: Valid loss: 0.8558 - accuracy: 0.1734\n",
      "Epoch 2: Train loss: 0.7817, - accuracy: 0.1925\n",
      "Epoch 2: Valid loss: 0.7146 - accuracy: 0.2065\n",
      "Epoch 3: Train loss: 0.6839, - accuracy: 0.2170\n",
      "Epoch 3: Valid loss: 0.6513 - accuracy: 0.2228\n",
      "Epoch 4: Train loss: 0.6364, - accuracy: 0.2288\n",
      "Epoch 4: Valid loss: 0.6198 - accuracy: 0.2301\n",
      "Epoch 5: Train loss: 0.6120, - accuracy: 0.2345\n",
      "Epoch 5: Valid loss: 0.6017 - accuracy: 0.2337\n",
      "Best val_acc: 0.2337365328516486\n"
     ]
    }
   ],
   "source": [
    "lr, emb_dim = 0.001, 20\n",
    "model = MatrixFactorization(emb_size, emb_dim).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "numEpoch = 5\n",
    "\n",
    "t_losses = []\n",
    "t_accs = []\n",
    "v_losses = []\n",
    "v_accs = []\n",
    "\n",
    "best_loss = 100.0\n",
    "best_acc = 0.0\n",
    "# Progress bar\n",
    "bar = display(progress(0, len(train_loader)), display_id=True)\n",
    "for epoch in range(numEpoch):\n",
    "\t\n",
    "\t# Train\n",
    "\tt_loss, t_acc = train(epoch, model)\n",
    "\tt_losses.append(t_loss)\n",
    "\tt_accs.append(t_acc)\n",
    "\tprint(f\"Epoch {epoch+1}: Train loss: {t_loss:.4f}, - accuracy: {t_acc:.4f}\")\n",
    "\n",
    "\t# Validate\n",
    "\tv_loss, v_acc = eval(epoch, model)\n",
    "\tv_losses.append(v_loss)\n",
    "\tv_accs.append(v_acc)\n",
    "\tprint(F\"Epoch {epoch+1}: Valid loss: {v_loss:.4f} - accuracy: {v_acc:.4f}\")\n",
    "\n",
    "\t# LR scheduler\n",
    "\t# scheduler.step()\n",
    "\n",
    "\t# Compare to current best accuracy\n",
    "\tif v_loss <= best_loss:\n",
    "\t\tbest_loss = v_loss\n",
    "\t\tbest_acc = v_acc\n",
    "\n",
    "print(f\"Best val_acc: {best_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two FC Layers\n",
    "class Linear(nn.Module):\n",
    "\tdef __init__(self, num_features = emb_size, embedding_dim = 20, hidden_dim = 32):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.emb = nn.EmbeddingBag(num_features, embedding_dim, max_norm=1, scale_grad_by_freq=True, padding_idx=0)\n",
    "\n",
    "\t\tself.lin1 = nn.Linear(2*embedding_dim, hidden_dim)\n",
    "\t\t#self.lin1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "\t\tself.lin2 = nn.Linear(hidden_dim, 1)\n",
    "\t\tself.lin2.bias = nn.Parameter(torch.full(self.lin2.bias.shape, 0.5))\n",
    "\t\tself.nonlin = nn.ReLU()\n",
    "\t\tself.drop = nn.Dropout(0.5)\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\t\tedge_index = data.edge_label_index\n",
    "\t\tuser1_feat, user2_feat = data.BoW[edge_index]\n",
    "\n",
    "\t\tuser1_vector = self.emb(user1_feat)\n",
    "\t\tuser2_vector = self.emb(user2_feat)\n",
    "\n",
    "\t\tx = torch.cat([user1_vector, user2_vector], dim=1)\n",
    "\t\t#x = user1_vector + user2_vector\n",
    "\t\t\n",
    "\t\tout = self.lin2(self.drop(self.nonlin(self.lin1(x)))).squeeze()\n",
    "\t\treturn out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='647'\n",
       "            max='647',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            647\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 0.6715, - accuracy: 0.4811\n",
      "Epoch 1: Valid loss: 0.6505 - accuracy: 0.4376\n",
      "Epoch 2: Train loss: 0.6386, - accuracy: 0.4170\n",
      "Epoch 2: Valid loss: 0.6267 - accuracy: 0.4318\n",
      "Epoch 3: Train loss: 0.6233, - accuracy: 0.4583\n",
      "Epoch 3: Valid loss: 0.6153 - accuracy: 0.4777\n",
      "Epoch 4: Train loss: 0.6160, - accuracy: 0.4812\n",
      "Epoch 4: Valid loss: 0.6096 - accuracy: 0.4805\n",
      "Epoch 5: Train loss: 0.6119, - accuracy: 0.4865\n",
      "Epoch 5: Valid loss: 0.6058 - accuracy: 0.4934\n",
      "Best val_acc: 0.4933773384149864\n"
     ]
    }
   ],
   "source": [
    "lr, emb_dim, hidden_dim = 0.1, 20, 256\n",
    "model = Linear(emb_size, emb_dim, hidden_dim).to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "numEpoch = 5\n",
    "\n",
    "t_losses = []\n",
    "t_accs = []\n",
    "v_losses = []\n",
    "v_accs = []\n",
    "\n",
    "best_loss = 100.0\n",
    "best_acc = 0.0\n",
    "# Progress bar\n",
    "bar = display(progress(0, len(train_loader)), display_id=True)\n",
    "for epoch in range(numEpoch):\n",
    "\t# Train\n",
    "\tt_loss, t_acc = train(epoch, model)\n",
    "\tt_losses.append(t_loss)\n",
    "\tt_accs.append(t_acc)\n",
    "\tprint(f\"Epoch {epoch+1}: Train loss: {t_loss:.4f}, - accuracy: {t_acc:.4f}\")\n",
    "\n",
    "\t# Validate\n",
    "\tv_loss, v_acc = eval(epoch, model)\n",
    "\tv_losses.append(v_loss)\n",
    "\tv_accs.append(v_acc)\n",
    "\tprint(F\"Epoch {epoch+1}: Valid loss: {v_loss:.4f} - accuracy: {v_acc:.4f}\")\n",
    "\n",
    "\t# LR scheduler\n",
    "\t# scheduler.step()\n",
    "\n",
    "\t# Compare to current best accuracy\n",
    "\tif v_loss <= best_loss:\n",
    "\t\tbest_loss = v_loss\n",
    "\t\tbest_acc = v_acc\n",
    "\n",
    "print(f\"Best val_acc: {best_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two CGN layers with reverse message passing, because the people we follow influence us\n",
    "# (The reverse is also true, but to a lesser degree)\n",
    "class GCN(torch.nn.Module):\n",
    "\tdef __init__(self, num_features=emb_size, embedding_dim = 20, hidden_dim = 32):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.emb = nn.EmbeddingBag(num_features,\n",
    "                                   embedding_dim,\n",
    "                                   max_norm=1,\n",
    "                                   scale_grad_by_freq=True,\n",
    "                                   mode=\"sum\",\n",
    "                                   padding_idx=0,\n",
    "                                   )\n",
    "\t\tself.n2v = n2v\n",
    "\t\tself.combine = nn.Linear(2*embedding_dim, embedding_dim)\n",
    "\n",
    "\t\tself.conv1 = gnn.GCNConv(n2v.embedding_dim, hidden_dim, flow=\"target_to_source\")\n",
    "\t\tself.conv2 = gnn.GCNConv(hidden_dim, hidden_dim, flow=\"target_to_source\")\n",
    "\n",
    "\t\tself.nonlin = nn.ReLU()\n",
    "\t\tself.drop = nn.Dropout(0.5)\n",
    "\t\t\n",
    "\t\tself.classifier = gnn.Linear(hidden_dim, 1)\n",
    "\t\t#self.classifier.bias = nn.Parameter(torch.full(self.classifier.bias.shape, 0.5))\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\t\tnode_index, node_bow, edge_index, label_index = data.n_id, data.BoW, data.edge_index, data.edge_label_index\n",
    "\t\t\n",
    "\t\t# Combine node embeddings of the bag of words and node2vec\n",
    "\t\tx1 = self.emb(node_bow)\n",
    "\t\tx2 = self.n2v(node_index)\n",
    "\t\tx = torch.cat([x1,x2], dim=1)\n",
    "\t\tx = self.combine(self.nonlin(x))\n",
    "\n",
    "\t\t# cuda dies here...\n",
    "\t\tout = self.conv1(x, edge_index)\n",
    "\t\tout = self.conv2(self.drop(self.nonlin(out)), edge_index)\n",
    "\n",
    "\t\tout = self.classifier(self.drop(self.nonlin(out))).squeeze()\n",
    "\t\tout = out[label_index[0]]\n",
    "\t\treturn out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='19'\n",
       "            max='1294',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            19\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48767, 20])\n",
      "cuda:0\n",
      "torch.Size([48988, 20])\n",
      "cuda:0\n",
      "torch.Size([48839, 20])\n",
      "cuda:0\n",
      "torch.Size([48878, 20])\n",
      "cuda:0\n",
      "torch.Size([48954, 20])\n",
      "cuda:0\n",
      "torch.Size([49021, 20])\n",
      "cuda:0\n",
      "torch.Size([49179, 20])\n",
      "cuda:0\n",
      "torch.Size([48470, 20])\n",
      "cuda:0\n",
      "torch.Size([49141, 20])\n",
      "cuda:0\n",
      "torch.Size([48766, 20])\n",
      "cuda:0\n",
      "torch.Size([48501, 20])\n",
      "cuda:0\n",
      "torch.Size([48925, 20])\n",
      "cuda:0\n",
      "torch.Size([48442, 20])\n",
      "cuda:0\n",
      "torch.Size([48687, 20])\n",
      "cuda:0\n",
      "torch.Size([48920, 20])\n",
      "cuda:0\n",
      "torch.Size([48560, 20])\n",
      "cuda:0\n",
      "torch.Size([49096, 20])\n",
      "cuda:0\n",
      "torch.Size([49063, 20])\n",
      "cuda:0\n",
      "torch.Size([49121, 20])\n",
      "cuda:0\n",
      "torch.Size([48731, 20])\n",
      "cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m bar \u001b[38;5;241m=\u001b[39m display(progress(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_loader)), display_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numEpoch):\n\u001b[0;32m     17\u001b[0m \t\u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \tt_loss, t_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \tt_losses\u001b[38;5;241m.\u001b[39mappend(t_loss)\n\u001b[0;32m     20\u001b[0m \tt_accs\u001b[38;5;241m.\u001b[39mappend(t_acc)\n",
      "Cell \u001b[1;32mIn[29], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch, model)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     29\u001b[0m \t\u001b[38;5;66;03m# Predict if user1 follows user2\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \t\u001b[38;5;66;03m# .edge_label_index -> edges to predict\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \t\u001b[38;5;66;03m# .edge_label -> 0-1 edge labels of edge_label_index\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \toptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 34\u001b[0m \toutputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \tloss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch\u001b[38;5;241m.\u001b[39medge_label\u001b[38;5;241m.\u001b[39mto(DEVICE))\n\u001b[0;32m     36\u001b[0m \tloss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Krisztián\\Documents\\Homework\\Mélytanulás\\Nagyházi\\Twitter_GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Krisztián\\Documents\\Homework\\Mélytanulás\\Nagyházi\\Twitter_GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[37], line 38\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(edge_index\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# cuda dies here...\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonlin(out)), edge_index)\n\u001b[0;32m     40\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonlin(out)))\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32mc:\\Users\\Krisztián\\Documents\\Homework\\Mélytanulás\\Nagyházi\\Twitter_GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Krisztián\\Documents\\Homework\\Mélytanulás\\Nagyházi\\Twitter_GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Krisztián\\Documents\\Homework\\Mélytanulás\\Nagyházi\\Twitter_GNN\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\Krisztián\\Documents\\Homework\\Mélytanulás\\Nagyházi\\Twitter_GNN\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:99\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[1;32m---> 99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    104\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Krisztián\\Documents\\Homework\\Mélytanulás\\Nagyházi\\Twitter_GNN\\lib\\site-packages\\torch_geometric\\utils\\loop.py:650\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[1;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, EdgeIndex):\n\u001b[0;32m    648\u001b[0m     is_undirected \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39mis_undirected\n\u001b[1;32m--> 650\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m \u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, EdgeIndex):\n\u001b[0;32m    653\u001b[0m     edge_index\u001b[38;5;241m.\u001b[39m_is_undirected \u001b[38;5;241m=\u001b[39m is_undirected\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "lr, emb_dim, hidden = 0.001, 20, 256\n",
    "model = GCN(emb_size, emb_dim, hidden).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "numEpoch = 5\n",
    "\n",
    "t_losses = []\n",
    "t_accs = []\n",
    "v_losses = []\n",
    "v_accs = []\n",
    "\n",
    "best_loss = 1000.0\n",
    "best_acc = 0.0\n",
    "# Progress bar\n",
    "bar = display(progress(0, len(train_loader)), display_id=True)\n",
    "for epoch in range(numEpoch):\n",
    "\t# Train\n",
    "\tt_loss, t_acc = train(epoch, model)\n",
    "\tt_losses.append(t_loss)\n",
    "\tt_accs.append(t_acc)\n",
    "\tprint(f\"Epoch {epoch+1}: Train loss: {t_loss:.4f}, - accuracy: {t_acc:.4f}\")\n",
    "\n",
    "\t# Validate\n",
    "\tv_loss, v_acc = eval(epoch, model)\n",
    "\tv_losses.append(v_loss)\n",
    "\tv_accs.append(v_acc)\n",
    "\tprint(F\"Epoch {epoch+1}: Valid loss: {v_loss:.4f} - accuracy: {v_acc:.4f}\")\n",
    "\n",
    "\t# LR scheduler\n",
    "\t# scheduler.step()\n",
    "\n",
    "\t# Compare to current best accuracy\n",
    "\tif v_loss <= best_loss:\n",
    "\t\tbest_loss = v_loss\n",
    "\t\tbest_acc = v_acc\n",
    "\n",
    "print(f\"Best val_acc: {best_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Something that actually works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obsolete code\n",
    "\n",
    "This was my original plan before settling on bag-of-words embeddings of the node features\n",
    "\n",
    "(Note: This would have resulted in ~71k (sparse) tensor files, which are not large, but slow to look up, and the graph would need too much memory anyways)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to tensor for function saving individual vectors\n",
    "def dict2vec(d, idx):\n",
    "\tvector = torch.zeros(len(allfeatnames)+1, dtype=torch.int64)\n",
    "\tfor k,v in d.items():\n",
    "\t\tvector[allfeatnames.loc[allfeatnames == k].index] = v\n",
    "\ttorch.save(vector.to_sparse(), f\"./Data/tensors/vector{idx}.pt\")\n",
    "\n",
    "# Helper function to load feature vectors by node ID\n",
    "def id2vec(idx):\n",
    "\tif os.path.isfile(f\"./Data/tensors/vector{idx}.pt\"):\n",
    "\t\treturn torch.load(f\"./Data/tensors/vector{idx}.pt\").to_dense().to(DEVICE)\n",
    "\tprint(\"Bad index\")\n",
    "\treturn torch.zeros(emb_size).to(DEVICE)\n",
    "\n",
    "# Huge computation, never run this again!\n",
    "#for idx in allfeats.index:\n",
    "#\tdict2vec(allfeats.loc[idx].item(), idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Twitter_GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
