{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -q\n",
    "\n",
    "# pip didn't find pyg-lib for some reason\n",
    "#!pip install pyg-lib>=0.4 torch-scatter>=2.1 torch-sparse>=0.6 torch-cluster>=1.6 torch-spline_conv>=1.2 -f https://data.pyg.org/whl/torch-2.5.0+cu121.html --force-reinstall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ego networks\n",
    "!wget -P /Data https://snap.stanford.edu/data/twitter.tar.gz\n",
    "!unzip -qq /Data/twitter.tar.gz\n",
    "!rm twitter.tar.gz\n",
    "\n",
    "# Edge list\n",
    "!wget -P /Data https://snap.stanford.edu/data/twitter_combined.txt.gz\n",
    "!unzip /Data/twitter_combined.txt.gz\n",
    "!rm twitter_combined.txt.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e58483e670>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Graph\n",
    "import networkx as nx\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100318079', '10146102', '101859065', '101903164', '102765423']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get nodeID's with associated files (ego nodes)\n",
    "nodes = []\n",
    "for file in os.listdir(\"./Data/twitter\"):\n",
    "\tif file.split(\".\")[0] not in nodes:\n",
    "\t\tnodes.append(file.split(\".\")[0])\n",
    "nodes[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 10 out of 973\n",
      "File 20 out of 973\n",
      "File 30 out of 973\n",
      "File 40 out of 973\n",
      "File 50 out of 973\n",
      "File 60 out of 973\n",
      "File 70 out of 973\n",
      "File 80 out of 973\n",
      "File 90 out of 973\n",
      "File 100 out of 973\n",
      "File 110 out of 973\n",
      "File 120 out of 973\n",
      "File 130 out of 973\n",
      "File 140 out of 973\n",
      "File 150 out of 973\n",
      "File 160 out of 973\n",
      "File 170 out of 973\n",
      "File 180 out of 973\n",
      "File 190 out of 973\n",
      "File 200 out of 973\n",
      "File 210 out of 973\n",
      "File 220 out of 973\n",
      "File 230 out of 973\n",
      "File 240 out of 973\n",
      "File 250 out of 973\n",
      "File 260 out of 973\n",
      "File 270 out of 973\n",
      "File 280 out of 973\n",
      "File 290 out of 973\n",
      "File 300 out of 973\n",
      "File 310 out of 973\n",
      "File 320 out of 973\n",
      "File 330 out of 973\n",
      "File 340 out of 973\n",
      "File 350 out of 973\n",
      "File 360 out of 973\n",
      "File 370 out of 973\n",
      "File 380 out of 973\n",
      "File 390 out of 973\n",
      "File 400 out of 973\n",
      "File 410 out of 973\n",
      "File 420 out of 973\n",
      "File 430 out of 973\n",
      "File 440 out of 973\n",
      "File 450 out of 973\n",
      "File 460 out of 973\n",
      "File 470 out of 973\n",
      "File 480 out of 973\n",
      "File 490 out of 973\n",
      "File 500 out of 973\n",
      "File 510 out of 973\n",
      "File 520 out of 973\n",
      "File 530 out of 973\n",
      "File 540 out of 973\n",
      "File 550 out of 973\n",
      "File 560 out of 973\n",
      "File 570 out of 973\n",
      "File 580 out of 973\n",
      "File 590 out of 973\n",
      "File 600 out of 973\n",
      "File 610 out of 973\n",
      "File 620 out of 973\n",
      "File 630 out of 973\n",
      "File 640 out of 973\n",
      "File 650 out of 973\n",
      "File 660 out of 973\n",
      "File 670 out of 973\n",
      "File 680 out of 973\n",
      "File 690 out of 973\n",
      "File 700 out of 973\n",
      "File 710 out of 973\n",
      "File 720 out of 973\n",
      "File 730 out of 973\n",
      "File 740 out of 973\n",
      "File 750 out of 973\n",
      "File 760 out of 973\n",
      "File 770 out of 973\n",
      "File 780 out of 973\n",
      "File 790 out of 973\n",
      "File 800 out of 973\n",
      "File 810 out of 973\n",
      "File 820 out of 973\n",
      "File 830 out of 973\n",
      "File 840 out of 973\n",
      "File 850 out of 973\n",
      "File 860 out of 973\n",
      "File 870 out of 973\n",
      "File 880 out of 973\n",
      "File 890 out of 973\n",
      "File 900 out of 973\n",
      "File 910 out of 973\n",
      "File 920 out of 973\n",
      "File 930 out of 973\n",
      "File 940 out of 973\n",
      "File 950 out of 973\n",
      "File 960 out of 973\n",
      "File 970 out of 973\n",
      "0                      \n",
      "1                     1\n",
      "2          10bandsilove\n",
      "3                     2\n",
      "4                    20\n",
      "              ...      \n",
      "155517    puraseduction\n",
      "155518           rrrafo\n",
      "155519       saumenscch\n",
      "155520      sheisunreal\n",
      "155521    twstdsymphony\n",
      "Length: 155521, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'coachella': 1, 'gabrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'charlieroseshow': 1, 'claychristensen': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'jack': 1, 'makeitwork'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'jack': 1, 'jbrewer': 1, 'twitter': 1, 'twitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'foodtruck': 1, 'gzahnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568552194</th>\n",
       "      <td>{'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627575</th>\n",
       "      <td>{'lucasgomezbr': 1, 'natystadler': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568655523</th>\n",
       "      <td>{'dailyprofet': 1, 'paixaojovem': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568699879</th>\n",
       "      <td>{'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568770231</th>\n",
       "      <td>{'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120707 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Features\n",
       "12         {'claychristensen': 1, 'coachella': 1, 'gabrie...\n",
       "12         {'charlieroseshow': 1, 'claychristensen': 1, '...\n",
       "12         {'claychristensen': 1, 'jack': 1, 'makeitwork'...\n",
       "12         {'jack': 1, 'jbrewer': 1, 'twitter': 1, 'twitt...\n",
       "12         {'claychristensen': 1, 'foodtruck': 1, 'gzahnd...\n",
       "...                                                      ...\n",
       "568552194  {'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...\n",
       "568627575              {'lucasgomezbr': 1, 'natystadler': 1}\n",
       "568655523               {'dailyprofet': 1, 'paixaojovem': 1}\n",
       "568699879  {'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...\n",
       "568770231  {'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...\n",
       "\n",
       "[120707 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All feat names in order of discovery\n",
    "allfeatnames = pd.Series(dtype=str)\n",
    "# Node features as dictionaries of followed #-s and @-s (with multiplicities)\n",
    "allfeats = pd.DataFrame(columns=[\"Features\"])\n",
    "c = 0\n",
    "\n",
    "for node in nodes:\n",
    "\tc+=1\n",
    "\tif not c%10:\n",
    "\t\tprint(f\"File {c} out of {len(nodes)}\")\n",
    "\n",
    "\t# Remove duplicates with special characters and lowercase-uppercase shenanigans\n",
    "\tfeatnames = pd.read_csv(f\"./Data/twitter/{node}.featnames\", sep=\" \", header=None)[1].str.rstrip(\".,;:'+-!?)(}{][\").str.lstrip(\"#@\").str.lower()\n",
    "\tallfeatnames = pd.Series(pd.concat([allfeatnames, featnames]).unique())\n",
    "\n",
    "\t# Feature vectors except ego node\n",
    "\tfeatdf = pd.read_csv(f\"./Data/twitter/{node}.feat\", sep=\" \", header=None).set_index(0).set_axis(featnames, axis=1)\n",
    "\n",
    "\t# Feature vector of ego node\n",
    "\tegofeat = pd.read_csv(f\"./Data/twitter/{node}.egofeat\", sep=\" \", header=None).set_axis(featnames, axis=1)\n",
    "\tegofeat = pd.concat([pd.DataFrame([int(node)], columns=[0]), egofeat], axis=1).set_index(0)\n",
    "\n",
    "\t# All feature vectors in a DataFrame\n",
    "\tfeatdf = pd.concat([egofeat, featdf])\n",
    "\tfeatdf = featdf.T.groupby(level=0).sum().T\n",
    "\n",
    "\t# Create bag-of-words style dictionaries to later turn into tensors\n",
    "\tfeatdf[\"Features\"] = [{col: featdf.at[idx, col] for col in featdf.columns if featdf.at[idx, col]} for idx in featdf.index]\n",
    "\t\n",
    "\t# Concatenate\n",
    "\tallfeats = pd.concat([allfeats, featdf.Features])\n",
    "\n",
    "\t# Edges - use twitter_combined instead\n",
    "\t# edgedf = pd.read_csv(f\"./Data/twitter/{node}.edges\", sep=\" \", header=None).T\n",
    "\t\n",
    "# Drop the (one) empty string and save\n",
    "allfeatnames = allfeatnames.dropna()\n",
    "allfeatnames.to_json(\"feature_names.json\")\n",
    "print(allfeatnames)\n",
    "\n",
    "# Drop the empty dicts (there's over 10.000 unique nodes following nothing)\n",
    "allfeats = allfeats.loc[allfeats[\"Features\"] != {}].sort_index()\n",
    "allfeats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Bag_of_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'coachella': 1, 'gabrie...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'amyquispe': 1, 'baratunde': 1, 'busterbenson...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568552194</th>\n",
       "      <td>{'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...</td>\n",
       "      <td>568552194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627575</th>\n",
       "      <td>{'lucasgomezbr': 1, 'natystadler': 1}</td>\n",
       "      <td>568627575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568655523</th>\n",
       "      <td>{'dailyprofet': 1, 'paixaojovem': 1}</td>\n",
       "      <td>568655523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568699879</th>\n",
       "      <td>{'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...</td>\n",
       "      <td>568699879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568770231</th>\n",
       "      <td>{'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...</td>\n",
       "      <td>568770231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70923 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Features  Bag_of_Words\n",
       "12         {'claychristensen': 1, 'coachella': 1, 'gabrie...            12\n",
       "13         {'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...            13\n",
       "17         {'amyquispe': 1, 'baratunde': 1, 'busterbenson...            17\n",
       "20         {'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...            20\n",
       "47         {'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...            47\n",
       "...                                                      ...           ...\n",
       "568552194  {'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...     568552194\n",
       "568627575              {'lucasgomezbr': 1, 'natystadler': 1}     568627575\n",
       "568655523               {'dailyprofet': 1, 'paixaojovem': 1}     568655523\n",
       "568699879  {'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...     568699879\n",
       "568770231  {'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...     568770231\n",
       "\n",
       "[70923 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# allfeats contains many duplicate indices which we need to deal with\n",
    "\n",
    "# Function to aggregate the dicts of duplicate indices, taking the maximum of all multiplicities\n",
    "# (The feature vectors are different across files)\n",
    "def dict_aggregation(dicts):\n",
    "\tout = {}\n",
    "\tfor d in dicts:\n",
    "\t\tfor k,v in d.items():\n",
    "\t\t\tif k in out.keys() and out[k] >= v:\n",
    "\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\tout[k] = v\n",
    "\treturn out\n",
    "\n",
    "# Update node dicts with duplicate indices, then remove unnecessary duplicates\n",
    "for idx in allfeats[allfeats.index.duplicated()].index.unique():\n",
    "\tagg = dict_aggregation(allfeats.loc[idx].Features)\n",
    "\tallfeats.loc[idx, \"Features\"] = [agg] * len(allfeats.loc[idx])\n",
    "\n",
    "allfeats = allfeats[~allfeats.index.duplicated()]\n",
    "allfeats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_size:  194\n",
      "About 2.48% done\n",
      "About 2.48% done\n",
      "About 2.49% done\n",
      "About 2.51% done\n",
      "About 2.59% done\n",
      "About 2.66% done\n",
      "About 2.70% done\n",
      "About 2.75% done\n",
      "About 2.78% done\n",
      "About 2.80% done\n",
      "About 3.01% done\n",
      "About 3.02% done\n",
      "About 3.03% done\n",
      "About 3.28% done\n",
      "About 3.30% done\n",
      "About 3.37% done\n",
      "About 3.53% done\n",
      "About 3.66% done\n",
      "About 3.77% done\n",
      "About 3.91% done\n",
      "About 4.31% done\n",
      "About 4.32% done\n",
      "About 4.34% done\n",
      "About 4.58% done\n",
      "About 4.65% done\n",
      "About 4.80% done\n",
      "About 4.80% done\n",
      "About 4.80% done\n",
      "About 5.63% done\n",
      "About 6.02% done\n",
      "About 6.95% done\n",
      "About 6.96% done\n",
      "About 7.76% done\n",
      "About 8.04% done\n",
      "About 8.38% done\n",
      "About 9.75% done\n",
      "About 10.26% done\n",
      "About 10.67% done\n",
      "About 12.20% done\n",
      "About 12.97% done\n",
      "About 13.15% done\n",
      "About 13.39% done\n",
      "About 14.10% done\n",
      "About 16.38% done\n",
      "About 17.40% done\n",
      "About 18.14% done\n",
      "About 22.18% done\n",
      "About 22.23% done\n",
      "About 22.46% done\n",
      "About 23.08% done\n",
      "About 23.96% done\n",
      "About 29.03% done\n",
      "About 34.05% done\n",
      "About 34.59% done\n",
      "About 34.84% done\n",
      "About 35.67% done\n",
      "About 37.26% done\n",
      "About 38.39% done\n",
      "About 42.28% done\n",
      "About 42.56% done\n",
      "About 49.01% done\n",
      "About 49.18% done\n",
      "About 53.97% done\n",
      "About 60.83% done\n",
      "About 63.43% done\n",
      "About 67.55% done\n",
      "About 87.73% done\n",
      "About 96.02% done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krisztián\\AppData\\Local\\Temp\\ipykernel_14860\\769440309.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[tensor([ 31517,     86, 115673,   6741,   6741,  25306,   7622,  25768,  13811,\n",
      "         115629,   7120,   1681,  45262,  24086,   8859,   7310,  45288,  45228,\n",
      "          25336,  25336,  31027,   2976,   2976,  19157,  88730,  24332,  38647,\n",
      "          25698,  51775,  59974,  60065,  60010,  60015,  39992,   3173,   3122,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0])\n",
      " tensor([ 3462, 11073, 24296, 24326, 24371,  3690, 27037,  7678,  2824, 46801,\n",
      "          4024, 24267, 60107, 51870, 99549, 99549, 32071,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0])\n",
      " tensor([70048,  6685, 24196, 70065, 25493, 17615, 11073, 25597, 51914, 24462,\n",
      "         70138, 24538,  6803,  6803, 51981,  6374,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0])\n",
      " ...\n",
      " tensor([14243, 56482,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0])\n",
      " tensor([   676,    306,    318,    318, 135889,    567,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0])\n",
      " tensor([14028, 14038, 47269, 47269,  5516,  3697, 14082,  2109,  2109,  9728,\n",
      "          9728, 19705,  5322, 29928, 12183,  2272, 14113,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0])                                  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  allfeats.loc[:, 'Bag_of_Words'] = allfeats.Bag_of_Words.apply(id2bow)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Bag_of_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'coachella': 1, 'gabrie...</td>\n",
       "      <td>[tensor(31517), tensor(86), tensor(115673), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...</td>\n",
       "      <td>[tensor(3462), tensor(11073), tensor(24296), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'amyquispe': 1, 'baratunde': 1, 'busterbenson...</td>\n",
       "      <td>[tensor(70048), tensor(6685), tensor(24196), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...</td>\n",
       "      <td>[tensor(24104), tensor(51836), tensor(25369), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...</td>\n",
       "      <td>[tensor(25362), tensor(25367), tensor(25368), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568552194</th>\n",
       "      <td>{'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...</td>\n",
       "      <td>[tensor(148053), tensor(130385), tensor(148073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627575</th>\n",
       "      <td>{'lucasgomezbr': 1, 'natystadler': 1}</td>\n",
       "      <td>[tensor(67889), tensor(67751), tensor(0), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568655523</th>\n",
       "      <td>{'dailyprofet': 1, 'paixaojovem': 1}</td>\n",
       "      <td>[tensor(14243), tensor(56482), tensor(0), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568699879</th>\n",
       "      <td>{'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...</td>\n",
       "      <td>[tensor(676), tensor(306), tensor(318), tensor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568770231</th>\n",
       "      <td>{'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...</td>\n",
       "      <td>[tensor(14028), tensor(14038), tensor(47269), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70923 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Features  \\\n",
       "12         {'claychristensen': 1, 'coachella': 1, 'gabrie...   \n",
       "13         {'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...   \n",
       "17         {'amyquispe': 1, 'baratunde': 1, 'busterbenson...   \n",
       "20         {'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...   \n",
       "47         {'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...   \n",
       "...                                                      ...   \n",
       "568552194  {'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...   \n",
       "568627575              {'lucasgomezbr': 1, 'natystadler': 1}   \n",
       "568655523               {'dailyprofet': 1, 'paixaojovem': 1}   \n",
       "568699879  {'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...   \n",
       "568770231  {'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...   \n",
       "\n",
       "                                                Bag_of_Words  \n",
       "12         [tensor(31517), tensor(86), tensor(115673), te...  \n",
       "13         [tensor(3462), tensor(11073), tensor(24296), t...  \n",
       "17         [tensor(70048), tensor(6685), tensor(24196), t...  \n",
       "20         [tensor(24104), tensor(51836), tensor(25369), ...  \n",
       "47         [tensor(25362), tensor(25367), tensor(25368), ...  \n",
       "...                                                      ...  \n",
       "568552194  [tensor(148053), tensor(130385), tensor(148073...  \n",
       "568627575  [tensor(67889), tensor(67751), tensor(0), tens...  \n",
       "568655523  [tensor(14243), tensor(56482), tensor(0), tens...  \n",
       "568699879  [tensor(676), tensor(306), tensor(318), tensor...  \n",
       "568770231  [tensor(14028), tensor(14038), tensor(47269), ...  \n",
       "\n",
       "[70923 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to create a Bag of Words tensor given an index\n",
    "def id2bow(index, bag_size = 195):\n",
    "\tbow = torch.zeros(bag_size, dtype=torch.int64)\n",
    "\tdictionary = allfeats.loc[index, \"Features\"]\n",
    "\tj = 0\n",
    "\tfor key,value in dictionary.items():\n",
    "\t\t# Accounts for multiplicities\n",
    "\t\tbow[j:j+value] = allfeatnames[allfeatnames == key].index.item()\n",
    "\t\tj += value\n",
    "\t# Occasional progress check\n",
    "\t# Note: Remaining index values are not uniformly distributed\n",
    "\tif not index%1000:\n",
    "\t\tprint(f\"About {index/5687702:.2f}% done\")\n",
    "\treturn bow\n",
    "\n",
    "# bag_size should be at least this (set one higher just to be sure)\n",
    "print(\"bag_size: \", allfeats.Features.apply(lambda x: sum(x.values())).max())\n",
    "\n",
    "# Set column same as index and apply id2bow\n",
    "allfeats[\"Bag_of_Words\"] = allfeats.index\n",
    "allfeats.loc[:, 'Bag_of_Words'] = allfeats.Bag_of_Words.apply(id2bow)\n",
    "allfeats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Bag_of_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'coachella': 1, 'gabrie...</td>\n",
       "      <td>[tensor(31517), tensor(86), tensor(115673), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...</td>\n",
       "      <td>[tensor(3462), tensor(11073), tensor(24296), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'amyquispe': 1, 'baratunde': 1, 'busterbenson...</td>\n",
       "      <td>[tensor(70048), tensor(6685), tensor(24196), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...</td>\n",
       "      <td>[tensor(24104), tensor(51836), tensor(25369), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...</td>\n",
       "      <td>[tensor(25362), tensor(25367), tensor(25368), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568552194</th>\n",
       "      <td>{'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...</td>\n",
       "      <td>[tensor(148053), tensor(130385), tensor(148073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627575</th>\n",
       "      <td>{'lucasgomezbr': 1, 'natystadler': 1}</td>\n",
       "      <td>[tensor(67889), tensor(67751), tensor(0), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568655523</th>\n",
       "      <td>{'dailyprofet': 1, 'paixaojovem': 1}</td>\n",
       "      <td>[tensor(14243), tensor(56482), tensor(0), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568699879</th>\n",
       "      <td>{'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...</td>\n",
       "      <td>[tensor(676), tensor(306), tensor(318), tensor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568770231</th>\n",
       "      <td>{'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...</td>\n",
       "      <td>[tensor(14028), tensor(14038), tensor(47269), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70923 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Features  \\\n",
       "12         {'claychristensen': 1, 'coachella': 1, 'gabrie...   \n",
       "13         {'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...   \n",
       "17         {'amyquispe': 1, 'baratunde': 1, 'busterbenson...   \n",
       "20         {'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...   \n",
       "47         {'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...   \n",
       "...                                                      ...   \n",
       "568552194  {'a_olivieri11': 1, 'atlanta_falcons': 1, 'dak...   \n",
       "568627575              {'lucasgomezbr': 1, 'natystadler': 1}   \n",
       "568655523               {'dailyprofet': 1, 'paixaojovem': 1}   \n",
       "568699879  {'hotmail.com': 1, 'lualone': 1, 'mileybrcom':...   \n",
       "568770231  {'civiksms': 1, 'danshair': 1, 'darkhugh': 2, ...   \n",
       "\n",
       "                                                Bag_of_Words  \n",
       "12         [tensor(31517), tensor(86), tensor(115673), te...  \n",
       "13         [tensor(3462), tensor(11073), tensor(24296), t...  \n",
       "17         [tensor(70048), tensor(6685), tensor(24196), t...  \n",
       "20         [tensor(24104), tensor(51836), tensor(25369), ...  \n",
       "47         [tensor(25362), tensor(25367), tensor(25368), ...  \n",
       "...                                                      ...  \n",
       "568552194  [tensor(148053), tensor(130385), tensor(148073...  \n",
       "568627575  [tensor(67889), tensor(67751), tensor(0), tens...  \n",
       "568655523  [tensor(14243), tensor(56482), tensor(0), tens...  \n",
       "568699879  [tensor(676), tensor(306), tensor(318), tensor...  \n",
       "568770231  [tensor(14028), tensor(14038), tensor(47269), ...  \n",
       "\n",
       "[70923 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save\n",
    "allfeats.to_pickle(\"feature_bow.pkl\")\n",
    "allfeats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a node2vec embedding of the graph nodes\n",
    "\n",
    "Need to create the data split first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Bag_of_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'claychristensen': 1, 'coachella': 1, 'gabrie...</td>\n",
       "      <td>[tensor(31517), tensor(86), tensor(115673), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...</td>\n",
       "      <td>[tensor(3462), tensor(11073), tensor(24296), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'amyquispe': 1, 'baratunde': 1, 'busterbenson...</td>\n",
       "      <td>[tensor(70048), tensor(6685), tensor(24196), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...</td>\n",
       "      <td>[tensor(24104), tensor(51836), tensor(25369), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...</td>\n",
       "      <td>[tensor(25362), tensor(25367), tensor(25368), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Features  \\\n",
       "12  {'claychristensen': 1, 'coachella': 1, 'gabrie...   \n",
       "13  {'brainpicker': 1, 'ev': 1, 'eventbrite': 1, '...   \n",
       "17  {'amyquispe': 1, 'baratunde': 1, 'busterbenson...   \n",
       "20  {'aaronsw': 1, 'abdur': 1, 'amac': 1, 'dustin'...   \n",
       "47  {'al3x': 1, 'alexandrak': 1, 'allspaw': 1, 'av...   \n",
       "\n",
       "                                         Bag_of_Words  \n",
       "12  [tensor(31517), tensor(86), tensor(115673), te...  \n",
       "13  [tensor(3462), tensor(11073), tensor(24296), t...  \n",
       "17  [tensor(70048), tensor(6685), tensor(24196), t...  \n",
       "20  [tensor(24104), tensor(51836), tensor(25369), ...  \n",
       "47  [tensor(25362), tensor(25367), tensor(25368), ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load files created previously\n",
    "\n",
    "# Number of different feature names for the embedding size\n",
    "emb_size = 155522\n",
    "\n",
    "allfeats = pd.read_pickle(\"feature_bow.pkl\")\n",
    "allfeats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete graph nodes and edges:  81306 1768149\n",
      "Restricted graph nodes and edges:  70923 1655299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 1655299], BoW=[70923, 195], num_nodes=70923)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create graph: from pandas to NetworkX to torch_geometric data object\n",
    "# Complete edge list\n",
    "edge_list = pd.read_csv(\"./Data/twitter_combined.txt\", sep=\" \", header=None)\n",
    "\n",
    "# Complete graph\n",
    "G = nx.from_pandas_edgelist(edge_list, 0, 1, create_using=nx.DiGraph)\n",
    "print(\"Complete graph nodes and edges: \", G.number_of_nodes(), G.number_of_edges())\n",
    "\n",
    "# Restrict to nonzero feature nodes\n",
    "G = G.subgraph(allfeats.index)\n",
    "print(\"Restricted graph nodes and edges: \", G.number_of_nodes(), G.number_of_edges())\n",
    "\n",
    "# Give each node its it's bag of words as attribute\n",
    "nx.set_node_attributes(G, {idx: allfeats.Bag_of_Words[idx].to(DEVICE) for idx in allfeats.index}, \"BoW\")\n",
    "\n",
    "# Create the graph with a bag of words as the only node attribute\n",
    "graph = from_networkx(G)\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data: Apply train-val-test masks and create dataloaders\n",
    "\n",
    "transform = RandomLinkSplit(num_val=0.1, num_test=0.1)\n",
    "train_data, val_data, test_data = transform(graph)\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=[20, 20],\n",
    "    neg_sampling=NegativeSampling(\"binary\"),\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,\n",
    "    num_neighbors=[20, 20],\n",
    "    neg_sampling=NegativeSampling(\"binary\"),\n",
    "    batch_size=2048,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data,\n",
    "    num_neighbors=[20, 20],\n",
    "    neg_sampling=NegativeSampling(\"binary\"),\n",
    "    batch_size=2048,\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node2vec training function\n",
    "def n2v_train(numEpoch = 5,\n",
    "\t\t\t  batch_size = 128,\n",
    "\t\t\t  lr = 0.001,\n",
    "\t\t\t  **kwargs\n",
    "\t\t\t  ):\n",
    "\tn2v = gnn.Node2Vec(**kwargs).to(DEVICE)\n",
    "\tloader = n2v.loader(batch_size=batch_size, shuffle=True)\n",
    "\toptimizer = optim.Adam(n2v.parameters(), lr=lr)\n",
    "\n",
    "\tn2v.train()\n",
    "\tfor epoch in range(numEpoch):\n",
    "\t\tepoch_loss = 0\n",
    "\t\tfor pos_rw, neg_rw in loader:\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss = n2v.loss(pos_rw.to(DEVICE), neg_rw.to(DEVICE))\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tepoch_loss += loss.item()\n",
    "\t\tprint(f\"Epoch {epoch+1} loss: {epoch_loss/len(loader):.4f}\")\n",
    "\treturn n2v\n",
    "\n",
    "node2vec_trained = n2v_train(numEpoch=100,\n",
    "\t\t\t\t\t\t\t batch_size=512,\n",
    "\t\t\t\t\t\t\t edge_index = train_data.edge_index,\n",
    "\t\t\t\t\t\t\t embedding_dim = 20,\n",
    "\t\t\t\t\t\t\t walk_length = 10,\n",
    "\t\t\t\t\t\t\t context_size = 10,\n",
    "\t\t\t\t\t\t\t walks_per_node = 500,\n",
    "\t\t\t\t\t\t\t )\n",
    "\n",
    "# Save\n",
    "torch.save(node2vec_trained.state_dict(), \"node2vec_trained.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obsolete code\n",
    "\n",
    "This was my original plan before settling on bag-of-words embeddings of the node features.<br>\n",
    "(Note: This would have resulted in ~71k (sparse) tensor files, which are not large, but slow to look up, and the graph would need too much memory anyways)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to tensor for function saving individual vectors\n",
    "def dict2vec(d, idx):\n",
    "\tvector = torch.zeros(len(allfeatnames)+1, dtype=torch.int64)\n",
    "\tfor k,v in d.items():\n",
    "\t\tvector[allfeatnames.loc[allfeatnames == k].index] = v\n",
    "\ttorch.save(vector.to_sparse(), f\"./Data/tensors/vector{idx}.pt\")\n",
    "\n",
    "# Helper function to load feature vectors by node ID\n",
    "def id2vec(idx):\n",
    "\tif os.path.isfile(f\"./Data/tensors/vector{idx}.pt\"):\n",
    "\t\treturn torch.load(f\"./Data/tensors/vector{idx}.pt\").to_dense().to(DEVICE)\n",
    "\tprint(\"Bad index\")\n",
    "\treturn torch.zeros(emb_size).to(DEVICE)\n",
    "\n",
    "# Huge computation, never run this again!\n",
    "for idx in allfeats.index:\n",
    "\tpass\n",
    "\tdict2vec(allfeats.loc[idx].item(), idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Twitter_GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
